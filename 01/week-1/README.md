# Introduction to Deep Learning
**Deep Learning's Impact:**
- Deep learning is revolutionizing various industries, including healthcare, education, agriculture, and transportation (e.g., self-driving cars).
- It's compared to the "new electricity," similar to how electrification transformed society.

![AI is the new electricity](./images/01-new-electricity.png)

**Course Overview:**
- The specialization consists of five courses designed to equip learners with practical deep learning skills.
- **Course 1:** Foundations of neural networks and deep learning, including building a cat recognizer.
- **Course 2:** Practical aspects of deep learning, such as hyperparameter tuning and optimization.
- **Course 3:** Structuring machine learning projects, focusing on best practices in the deep learning era.
- **Course 4:** Convolutional neural networks (CNNs) for image processing.
- **Course 5:** Sequence models (RNNs, LSTMs) for natural language processing and other sequential data.

**Learning Outcomes:**
- Learners will gain the ability to build and train deep neural networks.
- They will learn to optimize and troubleshoot deep learning models.
- They will understand how to structure machine learning projects effectively.
- They will be able to apply CNN's and RNN's to real world problems.

**Goal:**
- To empower learners to contribute to the development of an AI-powered society.
- To give the student the skills to confidently add deep learning to their resume.

### What is a Neural Network?
**Neural Networks Basics:**

- Deep learning involves training neural networks, which can be very large.
- A neural network is a function that maps inputs to outputs.
- A simple neural network can be visualized as a single neuron that takes an input (e.g., house size) and produces an output (e.g., house price).
- This single neuron implements a function, such as a **ReLU** (Rectified Linear Unit) function, which introduces non-linearity.

![Housing Price Prediction](./images/02-house-price-prediction.png)

**Building Larger Networks:**

- Larger neural networks are created by stacking multiple neurons together.
- These neurons can process multiple input features (e.g., size, bedrooms, zip code, wealth).
- Intermediate layers of neurons, called hidden units, learn complex relationships between the input features.
- Each hidden unit can take all input features as input, allowing the network to decide what each unit represents.
- The layers are described as "densely connected" because each input is connected to each hidden unit.

![Densely Connected Neural Networks](./images/03-large-neural-networks.png)

**Supervised Learning:**

- Neural networks excel at supervised learning, where the goal is to learn a mapping from inputs (x) to outputs (y) using labeled training data.
- The network learns to automatically discover the underlying patterns and relationships in the data.
- The network is given the inputs and the desired outputs, and the network adjusts its internal parameters to best match the desired output.
- The power of neural networks is that with enough data, they can learn very complex relationships.

### Supervised Learning with Neural Networks
**Supervised Learning Dominance:**

- Most of the economic value generated by neural networks stems from supervised learning.
- Supervised learning involves training a model to map inputs (x) to outputs (y) using labeled data.

**Key Applications:**
- **Online Advertising:** Predicting ad click-through rates, a highly lucrative application.  
- **Computer Vision:** Image recognition and tagging.  
- **Speech Recognition:** Transcribing audio to text.  
- **Machine Translation:** Translating text between languages.  
- **Autonomous Driving:** Object detection and scene understanding.

![Key Application](./images/04-supervised-learning.png)

**Neural Network Architectures:**

- Different architectures are suited for different data types:
    - Standard neural networks for structured data (e.g., real estate, advertising).
    - Convolutional Neural Networks (CNNs) for image data.
    - Recurrent Neural Networks (RNNs) for sequential data (e.g., audio, language).

![Neural Network Architectures](./images/05-neural-network-architectures.png)

**Structured vs. Unstructured Data:**

- **Structured data:** Organized data with well-defined features (e.g., databases).
- **Unstructured data:** Raw data like audio, images, and text.
- Neural networks have significantly improved computers' ability to process unstructured data.
- Although unstructured data applications are more publicized, structured data applications generate much economic value.

![Structured vs Unstructured Data](./images/06-data-types.png)

**Economic Impact:**

- Neural networks are creating vast economic value across various sectors.
- This includes improved advertising systems, recommendation engines, and data processing capabilities.

**Recent Rise:**

- While the underlying concepts have existed for decades, recent advancements in computing power and data availability have fueled the surge in neural network applications.
### Why is Deep Learning taking Off?
![Scale drives deep learning progress](./images/07-scale-drives-deep-learning.png)
**Data Availability:**

- The digitization of society has led to an explosion of data, exceeding the capabilities of traditional machine learning algorithms.
- Neural networks thrive on large datasets, with performance improving as data volume increases.
- The growth of labeled data, where inputs (x) and outputs (y) are both available, is crucial.

**Computational Power:**
- Training large neural networks requires significant computational resources.  
- Advances in hardware, such as GPUs, have enabled the training of much larger and more complex models.  
- "Scale" is a driving force, referring to both the size of neural networks and the amount of data.

**Algorithmic Innovations:**

- While scale is important, algorithmic improvements have also played a vital role.
- Innovations like the shift from sigmoid to ReLU activation functions have significantly accelerated training.  
- These algorithmic changes often focus on improving computational efficiency.
- The faster computation allows for faster experimentation and iteration, greatly improving research and development.

**Iterative Development:**
- Training neural networks is an iterative process of designing, implementing, and refining models.
- Faster computation speeds up this cycle, allowing for more experimentation and faster progress.
- The Deep Learning research community is constantly evolving and improving algorithms.
![Training Neural Network is Iterative](./images/08-iterative-process.png)

**Future Outlook:**

- The factors driving deep learning's success—data, computation, and algorithms—are expected to continue improving.
- This suggests that deep learning will continue to advance for years to come.